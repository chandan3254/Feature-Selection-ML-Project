{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection Techniques\n",
        "This means methods that help us choose the most important features (columns) from a dataset and remove the unnecessary or less useful ones.\n",
        "\n",
        "üîπ Forward Selection (Forward Elimination)\n",
        "\n",
        "Forward selection is a feature selection technique in which we start with no features in the model and then add features one by one. At each step, the feature that gives the most significant improvement in the model‚Äôs performance is included. The process stops when adding new features does not improve the model.\n",
        "\n",
        "key points:---\n",
        "\n",
        "*   Start with no features.\n",
        "*   Add one feature at a time ‚Üí the one that improves model performance the most.\n",
        "*   Keep adding until no improvement.\n",
        "\n",
        "Example:\n",
        "Dataset = [Age, Salary, Experience, Education]\n",
        "\n",
        "1.   Start with nothing.\n",
        "2.   Add \"Experience\" ‚Üí best result\n",
        "3.   Add \"Age\" ‚Üí model improves.\n",
        "4.   Add \"Education\" ‚Üí no improvement ‚Üí stop.\n",
        "\n",
        "     üëâ Final model = [Experience, Age]\n",
        "\n",
        "\n",
        "üîπ Backward Selection (Backward Elimination)\n",
        "\n",
        "Backward elimination is a feature selection technique in which we start with all the features in the model and then remove the least significant feature (based on p-value or contribution). This process continues until only the most important features remain in the model.\n",
        "\n",
        "key points:--\n",
        "\n",
        "\n",
        "*   Start with all features.\n",
        "*   Remove the least important one (statistically insignificant or weak).\n",
        "*  Keep removing until only important ones remain.\n",
        "\n",
        "Example:\n",
        "Dataset = [Age, Salary, Experience, Education]\n",
        "\n",
        "\n",
        "\n",
        "1.   Start with all.\n",
        "2.   \"Education\" is least useful ‚Üí remove.\n",
        "3.   \"Salary\" not significant ‚Üí remove.\n",
        "\n",
        "      üëâ Final model = [Age, Experience]\n",
        "\n",
        "simple summary\n",
        "‚ñ∂Forward = Start empty ‚Üí keep adding features.\n",
        "‚óÄBackward = Start full ‚Üí keep removing features.\n",
        "\n",
        "\n",
        "\n",
        "for exmple :---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YrC1gNyr5Ona"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NhcW2g6697N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a simple dataset"
      ],
      "metadata": {
        "id": "xmglpVsd99vH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = {\n",
        "    \"Age\":     [25, 30, 35, 40, 45],\n",
        "    \"Experience\": [1, 3, 5, 7, 9],\n",
        "    \"Education\":  [10, 12, 14, 16, 18],\n",
        "    \"Salary\":  [20, 40, 60, 80, 100]   # Target variable\n",
        "}\n",
        "df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "HpJZ39te-A5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Backward Elimination (using statsmodels)"
      ],
      "metadata": {
        "id": "keniAIXI-c_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "X = df[[\"Age\", \"Experience\", \"Education\"]]  # Features\n",
        "y = df[\"Salary\"]                           # Target\n",
        "\n",
        "# Add constant for intercept\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit model\n",
        "model = sm.OLS(y, X).fit()\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "NKBil-Ks-eah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ Forward Selection (using sklearn)"
      ],
      "metadata": {
        "id": "TulwPPmiED4C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "X = df.drop(\"Salary\", axis=1)\n",
        "y = df[\"Salary\"]\n",
        "\n",
        "selected_features = []\n",
        "remaining_features = list(X.columns)\n",
        "best_score = -1\n",
        "\n",
        "while remaining_features:\n",
        "    scores = []\n",
        "    for feature in remaining_features:\n",
        "        model = LinearRegression()\n",
        "        temp_features = selected_features + [feature]\n",
        "        model.fit(X[temp_features], y)\n",
        "        score = r2_score(y, model.predict(X[temp_features]))\n",
        "        scores.append((score, feature))\n",
        "\n",
        "    scores.sort(reverse=True)\n",
        "    if scores[0][0] > best_score:\n",
        "        best_score, best_feature = scores[0]\n",
        "        selected_features.append(best_feature)\n",
        "        remaining_features.remove(best_feature)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "print(\"Selected Features (Forward Selection):\", selected_features)\n"
      ],
      "metadata": {
        "id": "e50QYuzrEE1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "output is \"Experience\"\n",
        "\n",
        "1.   Forward Selection starts with nothing.\n",
        "2.   It checks: ‚ÄúWhich single feature predicts Salary best?‚Äù\n",
        "3.   In your data, Experience alone explains Salary almost perfectly.\n",
        "4.   When it tries to add Age or Education, the model does not get much better.\n",
        "5.   So it stops and keeps only Experience.\n",
        "\n",
        "üëâ Meaning: In your dataset, Experience is the strongest feature, so Forward Selection chooses only that.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eOjTtxcJEmGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ÄúNow I applied feature selection techniques on my original dataset stored in Google Drive.‚Äù"
      ],
      "metadata": {
        "id": "FH9i3FcjF6LM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector\n",
        "url = \"https://docs.google.com/spreadsheets/d/1Mdlhhgd7ViNVAsktM133-jBuOOCLgvxg/export?format=xlsx\"\n",
        "dataset = pd.read_excel(url)\n",
        "print(dataset.head(5))"
      ],
      "metadata": {
        "id": "agpO6n3AICm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ‚úÖ Load dataset\n",
        "url = \"https://docs.google.com/spreadsheets/d/1Mdlhhgd7ViNVAsktM133-jBuOOCLgvxg/export?format=xlsx\"\n",
        "dataset = pd.read_excel(url)\n",
        "\n",
        "X = dataset.iloc[:, :-1]\n",
        "y = dataset[\"Outcome\"]\n",
        "\n",
        "# ‚úÖ Logistic Regression Model\n",
        "lr = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# ‚úÖ Sequential Feature Selector\n",
        "fs = SequentialFeatureSelector(\n",
        "    lr,\n",
        "    k_features=5,\n",
        "    forward=True,\n",
        "    scoring='accuracy',\n",
        "    cv=5\n",
        ")"
      ],
      "metadata": {
        "id": "mURtYUGHPvco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the features are selected, we can create a pie chart to see which features have been chosen and what their ratio looks like."
      ],
      "metadata": {
        "id": "4cQwcMs9SXgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Fit the model\n",
        "fs = fs.fit(X, y)\n",
        "\n",
        "# ‚úÖ Selected & Not Selected Features\n",
        "selected = [X.columns[i] for i in fs.k_feature_idx_]\n",
        "not_selected = [col for col in X.columns if col not in selected]\n",
        "\n",
        "print(\"Selected Features:\", selected)\n",
        "\n",
        "# ‚úÖ Pie Chart\n",
        "labels = selected + not_selected\n",
        "sizes = [1 if col in selected else 0 for col in labels]\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "plt.title(\"Feature Selection Ratio\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wirUavDy8tvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚ôêThis will display a pie chart with:\n",
        "\n",
        "Selected Features ('Glucose', 'Blood_preser', 'Skin thikness', 'BMI', 'Age')\n",
        "\n",
        "Not Selected Features (along with their names)"
      ],
      "metadata": {
        "id": "W4KEzVelThMs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your pie chart is showing that 100% features are selected and 0% are not selected.\n",
        "This means the feature selection method (Forward Selection) decided to keep all the features from your dataset because they were all useful for predicting the output."
      ],
      "metadata": {
        "id": "ks6HHPv1T-YF"
      }
    }
  ]
}